{"cells":[{"source":"<H1>Latent Class Analysis</H1>","metadata":{"id":"bA5ajAmk7XH6"},"id":"12b1a83d-0667-41fc-a00d-65695fb413f7","cell_type":"markdown"},{"source":"<h3>This algorithm performs parameter estimation using expectation maximization to cluster a dataset of binary variables.</h3>","metadata":{},"id":"227cb53e-9760-4b3d-9950-2c3d7906549e","cell_type":"markdown"},{"source":"By \"latent class\" we mean \"unspecified class\" i.e. the dataset is missing a column specifying the class type of each row. <br>\n\nThe estimated parameters of the model consist of two types. <br>\nThe first set of parameters are percentages (probabilities) that describe what proportion of the data population belongs to each cluster (latent class).<br>\n(This is a list with one probability / percentage / proportion per latent class)<br><br>\nThe second set of parameters are probabilities that each variable column will contain a value of 1 for each latent class. <br>\n(This is a table with a row count equal to the number of latent classes and a column count equal to the number of columns in the input dataset)<br><br>\n \n It is assumed the original dataset is made up of patterns that can be clustered.<br>\n The algorithm will find the parameters by iteratively evaluating the dataset until a minimum amount of improvement is measured or until a hard limit of iterations occurs.<br>\n Once complete the estimated parameters are used in the predict method to assign new rows to one of the latent classes or alternatively with predict_proba assign a probability for each latent class for each row.<br><br>\n Because there is noise in the data the true assignment for every row is not possible.<br>\n e.g. a process for latent class A that normally generates a row like 1,1,1,1,1 will rarely<br> instead generate a row like 1,0,0,0,1 and if 1,0,0,0,1 is an exact or near match for a latent class B then that row will be assigned to B.<br>\n This means that the estimated parameters that are learned will not exactly match the true proportions of each class as long as there is uncertainty / variablility in the dataset.\n","metadata":{},"id":"8f1eb2d7-2803-4e62-9d98-9c00ed21872c","cell_type":"markdown"},{"source":"https://www.sciencedirect.com/science/article/pii/S0022202X2031575X\n<br><i>\nThere are two sets of parameters in an LatentClassAnalysis. <br>\nThe first is the set of inclusion probabilities (or class membership probabilities), that is, <br>\nthe probability that any random case in a population will be included in any LC. <br>\n<br>\nThe second parameter is the conditional probability that, given a specific class, a variable takes a certain value.<br>\n</i>","metadata":{},"id":"999cae35-d7cd-4d8f-8e0a-e3ebd03e5740","cell_type":"markdown"},{"source":"![image-caption](LCA.jpg)","metadata":{},"id":"6c8e0707-4adc-4a49-991b-81e648964e83","cell_type":"markdown"},{"source":"import numpy as np\nimport pandas as pd\nfrom collections import Counter\nimport math\nfrom numpy.random import Generator, PCG64\nimport scipy.stats as stats","metadata":{},"id":"ee7f357d-5b5e-40d3-bf21-a95c353a9a71","cell_type":"code","execution_count":1,"outputs":[]},{"source":"class LatentClassAnalysis:\n    \"\"\"\n    This class uses expectation maximization to estimate parameters for predicting the proportion of rows\n    in the dataset that belong to each latent class and the probability that each column in the dataset will\n    have a value of 1 for each of the latent classes.\n\n    The number of latent classes can be specified using the configuration parameter n_classes or if n_classes\n    is not specified then class_count_method, diff_percentile_threshold, and least_probable_threshold can be\n    set to cause the n_classes to be determined automatically using the method \"get_classes_by_prevalence\".\n\n    The learning_rate_threshold parameter can be set to cause the iterations to end early if the learning rate\n    slows to this threshold as measured by the difference between the summed change in the estimated parameters.\n    max_iterations acts as the hard cut off for iterations even if the learning rate threshold has not been hit.\n\n    verbose_level controls the amount of detail in logging.\n    random_state is passed into all internal pseudo-randomization methods for reproducibility.\n\n    \"\"\"\n    def __init__(self, init_data=None, n_classes=0, learning_rate_threshold=0.001, max_iterations=100,\n                 class_count_method='average_class_count', diff_percentile_threshold=99,\n                 least_probable_threshold=1, verbose_level=0, random_state=None):\n        self.n_classes = n_classes\n        self.class_count_method = class_count_method\n        self.learning_rate_threshold = learning_rate_threshold\n        self.learning_rate_threshold_hit = False\n        self.max_iterations = max_iterations\n        self.max_iterations_hit = False\n        self.diff_percentile_threshold = diff_percentile_threshold\n        self.least_probable_threshold = least_probable_threshold\n        self.verbose_level = verbose_level\n        self.random_state = random_state\n\n\n        self.iteration_count = 0\n\n        # model parameters\n        self.change_scores = [-np.inf]\n        # A list of proportions for each latent class from the entire population\n        self.population_to_class_probability = None\n        # Each row is a latent class and each column is a variable column from the dataset\n        self.variable_to_class_probability = None\n\n        self.classes_by_prevalence = None\n        self.median_class_count = 0\n        self.average_class_count = 0\n        self.expectations = None\n\n    def lookup_prob_for_each_variable_value_to_class(self, data, C):\n        # Each 1 or 0 has a probability value\n        # variable_to_class_probability defines the probability of a 1 in each variable position\n        # stats.bernoulli.pmf gives the probability in variable_to_class_probability if the value is a 1\n        # stats.bernoulli.pmf gives the 1 - probability in variable_to_class_probability if the value is a 0\n        return stats.bernoulli.pmf(data, p=self.variable_to_class_probability[C])\n\n    def calc_expectation(self, data):\n        \"\"\"\n        Calculates the newest probability that each row belongs to each class.\n        *** This is equivalent to Mixture Model EM where during each iteration each data row is assigned to a distribution it\n        is most likely to belong to so that distribution can get its summary statistics (e.g. mean and stdev)\n        recalculated with the rows assigned to it.\n        Here instead of assigning rows to a class we just store the probability for each class that each row belongs to it. ***\n        :param data: The population of data rows to learn from\n        :return: One row for each data row where each column is the probability that the data row belongs to a latent class.\n        \"\"\"\n        row_to_class_probabilities = np.zeros(shape=(data.shape[0], self.n_classes))\n\n        for C in range(self.n_classes):\n            # Calc the joint probability (multiply the probabilities together) for each row\n            per_row_variable_to_class_joint_probability = np.prod(self.lookup_prob_for_each_variable_value_to_class(data, C), axis=1)\n            # For each data row multiply the probability for Any row in the population to belong to class C by the\n            # joint probability of each variable column value to belong to class C\n            per_row_proba_for_class = self.population_to_class_probability[C] * per_row_variable_to_class_joint_probability\n            # Persist the probability that each row belongs to each Class\n            row_to_class_probabilities[:, C] = per_row_proba_for_class\n        # Normalize the probabilities (so they sum to 1.0) across Classes for each row by dividing each Class column probability\n        # by the sum total of all Class columns for each row\n        normalization_value = np.sum(row_to_class_probabilities, axis=1)\n        # Repeat and reshape\n        normalization_value = np.tile(normalization_value, (self.n_classes, 1)).T\n\n        return row_to_class_probabilities / normalization_value\n\n    def calc_and_cache_expectations(self, data):\n\n        self.expectations = self.calc_expectation(data)\n\n    def calc_maximization(self, data):\n\n\n        # Overwrite the population_to_class_probability values with the normalized calculated expectation probabilities\n        for C in range(self.n_classes):\n            self.population_to_class_probability[C] = np.sum(self.expectations[:, C]) / float(data.shape[0])\n\n        # Overwrite the variable_to_class_probability values with the normalized calculated expectation probabilities\n        # for each variable column to each Class.\n        for C in range(self.n_classes):\n            # Take a single column as a series for this one Class.\n            # This is the aggregated probability that each row belongs to the Class.\n            rows_probability_for_class = self.expectations[:, C]\n            # Reshape the series into a table with a single column so it can be multiplied against another table of columns.\n            rows_probability_for_class_reshaped = rows_probability_for_class[:, np.newaxis]\n            # Multiply the data rows of binary data against the single column of Class probabilities.\n            # This is where the \"maximization\" part occurs.\n            # Columns with 1s keep the row's probability of class membership and columns with 0s just get zero values.\n            class_probabilities_foreach_variable_column = rows_probability_for_class_reshaped * data\n            # Then we sum up all these probabilities where 1s existed in each column across all rows\n            sum_total_across_all_rows_foreach_column = np.sum(class_probabilities_foreach_variable_column, axis=0)\n            class_proba_forall_rows = np.sum(rows_probability_for_class)\n            # This is the aggregated probability that each variable column belongs to the Class across all rows.\n            # In variable_to_class_probability each row represents a Class and\n            # each value is the probability of the variables for that Class.\n            self.variable_to_class_probability[C] = np.minimum(sum_total_across_all_rows_foreach_column / class_proba_forall_rows, 1.0)\n\n    def fit(self, data):\n\n        self.learning_rate_threshold_hit = False\n        self.max_iterations_hit = False\n\n        # If no class count was passed in during initialization then scan the data to determine a class count automatically\n        if self.n_classes == 0:\n            if self.class_count_method:\n                self.classes_by_prevalence = self.get_classes_by_prevalence(data)\n                if self.class_count_method == 'median_class_count':\n                    self.n_classes = self.median_class_count\n                elif self.class_count_method == 'average_class_count':\n                    self.n_classes = self.average_class_count\n                else:\n                    self.n_classes = len(self.classes_by_prevalence[self.class_count_method])\n            else:\n                raise ValueError(\n                    'Either initialize with the number of classes or a class_count_method to automatically determine the number of classes.')\n\n        row_count, column_count = np.shape(data)\n\n        # Initialize the population\n        rng = Generator(PCG64(self.random_state))\n        self.population_to_class_probability = np.array(rng.gamma(0.5, 1, size=self.n_classes))\n        # Renormalize\n        self.population_to_class_probability = self.population_to_class_probability / self.population_to_class_probability.sum()\n\n        # Initialize variable to class probabilities\n        init_class_probs = []\n        for c in range(self.n_classes):\n            lst = np.array(rng.gamma(0.5, 1, size=column_count))\n            # Renormalize\n            lst = lst / lst.sum()\n            init_class_probs.append(lst.tolist())\n        self.variable_to_class_probability = np.array(init_class_probs)\n\n        import time\n        times = []\n\n        for i in range(self.max_iterations):\n            t0 = time.perf_counter()\n\n            self.iteration_count += 1\n\n            # Calculate expected probability assignments to each Class for each row\n            # and cache the resulting probability assignments in self.expectations\n            self.calc_and_cache_expectations(data)\n\n            # Update current population and variable probability to class maps\n            self.calc_maximization(data)\n\n            # Stop when change in learning has dropped below learning_rate_threshold or hits max_iterations\n            current_probabilities = np.zeros(shape=(row_count, self.n_classes))\n            # Sum up current probabilities into a score to compare to the previous score for the change rate\n            for C in range(self.n_classes):\n                variable_to_class_joint_probability = self.lookup_prob_for_each_variable_value_to_class(data, C)\n                variable_to_class_joint_probability = np.prod(variable_to_class_joint_probability, axis=1)\n                current_probabilities[:, C] = self.population_to_class_probability[C] * variable_to_class_joint_probability\n\n            # Use a log transform to allow learning_rate_threshold to be set somewhere around 1 or less as a logical range\n            change_score = np.sum(np.log(np.sum(current_probabilities, axis=1)))\n\n            if np.abs(change_score - self.change_scores[-1]) < self.learning_rate_threshold:\n                self.learning_rate_threshold_hit = True\n                break\n            else:\n                self.change_scores.append(change_score)\n                if i == self.max_iterations - 1:\n                    self.max_iterations_hit = True\n                    break\n\n            t1 = time.perf_counter()\n            times.append(t1-t0)\n\n        avg = np.array(times).mean()\n        total = np.array(times).sum()\n        if self.verbose_level >= 1:\n            print('\\n===================================================== fit run timings')\n            print('avg run time per loop: ', avg)\n            print('total run time: ', total)\n\n            print('\\n===================================================== fit state')\n            print('iteration count:', self.iteration_count)\n            print('max_iterations_hit:', self.max_iterations_hit)\n            print('learning_rate_threshold_hit:', self.learning_rate_threshold_hit)\n            print('n_classes:', self.n_classes)\n\n\n\n    def predict(self, data):\n        return np.argmax(self.predict_proba(data), axis=1)\n\n    def predict_proba(self, data):\n        return self.calc_expectation(data)\n\n    def get_classes_by_prevalence(self, data):\n        \"\"\"\n        This method gets a count of each unique pattern of 1s and 0s across all rows,\n        and then uses a few different methods to estimate the cut off in the sorted list of counts.\n\n        A variable on the class called median_class_count is set that is the median of all the different estimates.\n\n        Then the estimates for each method are returned.\n\n        The different methods used are:\n\n        largest_diff - Find the largest jump in count difference in the sorted count list.\n        least_probable_diff - Compute a list of probabilities from the list of differences using a normal distribution and find the least probable difference from previous counts in the sorted list.\n        diff_percentile - Take 1st matching index greater than the Xth percentile diff specified in diff_percentile_threshold\n        least_probable_percentile - Take 1st matching index of the Xth percentile least diff probability.\n\n\n\n        :param data: The original dataset used to get counts for each unique combination of column values\n        :return: A dictionary containing a best guess of class count for each method implemented.\n        \"\"\"\n\n        if self.verbose_level > 0:\n            print('** Calculating classes by prevalence **')\n\n        # Create a dictionary of variable pattern keys to their count values sorted largest count to smallest\n        counter = Counter(map(tuple, data))\n        counts = dict(counter)\n        counts = dict(sorted(counts.items(), key=lambda item: item[1], reverse=True))\n\n        # Transform the dictionary into a list of tuples to iterate over and lookup by indexes\n        if self.verbose_level >= 2:\n            print('\\n===================================================== Class keys and counts')\n        lstKeyValueMap = []\n        for k, v in counts.items():\n            lstKeyValueMap.append((k, v))\n            if self.verbose_level >= 2:\n                print(k, v)\n\n        # Reverse the list to be smallest to largest\n        rev_lstKeyValueMap = list(reversed(lstKeyValueMap))\n\n        # Iterate over the list of tuples (pattern key, count)\n        # Start with the first two items to initialize a normal distribution\n        # For each new item count value we will get a probability that it belongs to the normal distribution of all the items that came before\n        counter = 2\n        diff = max(rev_lstKeyValueMap[1][1] - rev_lstKeyValueMap[0][1], 0)\n\n        # First two items have the same average diff\n        threshold_diffs = [diff, diff]\n        for i, item in enumerate(rev_lstKeyValueMap[1:-2]):\n            next = rev_lstKeyValueMap[i + 2][1]\n\n            d = item[1]\n            diff = next - d\n\n            threshold_diffs.append(diff)\n            counter += 1\n        if self.verbose_level >= 3:\n            print('\\n===================================================== Sorted diff counts between class keys')\n            print(sorted(threshold_diffs))\n\n        counter = 2\n        summed = (threshold_diffs[0] + threshold_diffs[1])\n        mean = summed / counter\n        std = np.std([item for item in threshold_diffs[:counter]])\n        std = max(std, 1)\n        # First two items have 100% probability\n        threshold_probs = [1, 1]\n        stats_to_print = []\n        for i, item in enumerate(threshold_diffs[2:]):\n            d = item\n            if mean == 0 and std == 0:\n                threshold_prob = 1\n            else:\n                threshold_prob = stats.norm.pdf(d, loc=mean, scale=std)\n\n            if self.verbose_level >= 4:\n                stats_to_print.append([d, round(threshold_prob, 8), round(mean, 4), round(std, 4)])\n\n            threshold_probs.append(threshold_prob)\n            counter += 1\n            # Update the normal distribution parameters to include this new item for the next evaluation loop\n            summed = sum([item for item in threshold_diffs[:counter]])\n            mean = summed / counter\n            std = np.std([item for item in threshold_diffs[:counter]])\n\n        if self.verbose_level >= 4:\n            pd.set_option('display.max_columns', len(stats_to_print[0]))\n            pd.set_option('display.max_rows', 30)\n            print('\\n===================================================== least_probable_diff stats')\n            df = pd.DataFrame(stats_to_print, columns=['diff', 'probability', 'mean', 'std']).set_index('diff')\n            print(df.head(5))\n            print(df.tail(15))\n\n        results = {}\n\n        # 1st matching index of largest diff\n        threshold_idx = np.argmax(threshold_diffs)\n        results['largest_diff'] = rev_lstKeyValueMap[threshold_idx:]\n\n        # Get the index of the item least probable to belong to the normal distribution of all the items that came before\n        # All items above this threshold are considered clusters/distribution peaks/classes\n        # 1st matching index for least probable diff\n        threshold_idx = np.argmin(threshold_probs)\n        results['least_probable_diff'] = rev_lstKeyValueMap[threshold_idx:]\n\n        # 1st matching index greater than the Xth percentile diff specified in diff_percentile_threshold\n        threshold_idx = np.argmax(threshold_diffs > np.percentile(threshold_diffs, [self.diff_percentile_threshold]))\n        results['diff_percentile'] = rev_lstKeyValueMap[threshold_idx:]\n\n        # 1st matching index of the Xth percentile least diff probability\n        threshold_idx = np.argmax(threshold_probs < np.percentile(threshold_probs, [self.least_probable_threshold]))\n        results['least_probable_percentile'] = rev_lstKeyValueMap[threshold_idx:]\n\n        self.median_class_count = math.ceil(np.median([len(classes) for classes in results.values()]))\n        self.average_class_count = math.ceil(np.mean([len(classes) for classes in results.values()]))\n\n        if self.verbose_level >= 4:\n            print('\\n===================================================== Classes by prevalence')\n            for k, v in results.items():\n                print(k)\n                for class_key in v:\n                    print(class_key)\n\n        if self.verbose_level >= 2:\n            print('\\n===================================================== Class count method stats')\n            print('max count diff between class keys:', max(threshold_diffs))\n            print('diff_percentile:', self.diff_percentile_threshold, '%')\n            print('least_probable_percentile:', self.least_probable_threshold, '%')\n            print('median class count:', self.median_class_count)\n            print('average class count:', self.average_class_count)\n            for k, v in results.items():\n                print(k, 'class count:', len(v))\n\n\n        return results","metadata":{},"id":"8bf6f420-6178-4994-964d-d2bdf17af733","cell_type":"code","execution_count":2,"outputs":[]},{"source":"<h2>Testing the class</h2>","metadata":{},"id":"4f6014b7-d45f-4ec9-8802-52a0763822a2","cell_type":"markdown"},{"source":"Generate sample data with known starting probabilities which can be used to compare with the estimated parameters determined by the class.","metadata":{},"id":"38c50dee-7518-49a3-940f-cbcbe499c9bb","cell_type":"markdown"},{"source":"# Probability properties to generate training data\ntrue_variable_to_class_probability = [\n    [0.01, 0.40, 0.90, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n    [0.04, 0.90, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n    [0.90, 0.90, 0.05, 0.90, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n    [0.01, 0.02, 0.30, 0.01, 0.01, 0.90, 0.01, 0.02, 0.01, 0.01],\n    [0.01, 0.01, 0.01, 0.03, 0.80, 0.01, 0.01, 0.02, 0.01, 0.01],\n    [0.01, 0.02, 0.01, 0.01, 0.02, 0.01, 0.90, 0.01, 0.01, 0.01],\n    [0.01, 0.01, 0.01, 0.03, 0.90, 0.01, 0.01, 0.90, 0.01, 0.01],\n    [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.90, 0.90]\n]\ntrue_population_to_class_probability = [0.05, 0.08, 0.22, 0.14, 0.20, 0.14, 0.12, 0.05]\ngen_row_count = 12000\nseed = 1\nvariable_count = len(true_variable_to_class_probability[0])\n\n# Generate training data\ntraining_data = []\n\ndfAll = pd.DataFrame(columns=[0,1,2,3,4,5,6,7,8,9])\n\n# Iterate over per class properties\nfor pop_to_class, var_to_class in zip(true_population_to_class_probability, true_variable_to_class_probability):\n    row_count_by_percentage = int(pop_to_class * gen_row_count)\n    rows = stats.bernoulli.rvs(p=var_to_class, size=(row_count_by_percentage, variable_count),\n                               random_state=seed).tolist()\n    df = pd.DataFrame(rows)\n    # Get rid of rows that are all zeros\n    df = df.loc[~(df == 0).all(axis=1)]\n    rows = df.to_numpy().tolist()\n    training_data.extend(rows)\n    dfAll = dfAll.append(df)\n\ntraining_data = np.array(training_data)\n\nprint(dfAll.info(verbose=True))\nprint(dfAll.sample(20))","metadata":{"outputsMetadata":{"0":{"height":616,"type":"stream"}}},"id":"64ae785b-1454-45d5-a7e4-971c1435848f","cell_type":"code","execution_count":3,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 11167 entries, 0 to 599\nData columns (total 10 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   0       11167 non-null  object\n 1   1       11167 non-null  object\n 2   2       11167 non-null  object\n 3   3       11167 non-null  object\n 4   4       11167 non-null  object\n 5   5       11167 non-null  object\n 6   6       11167 non-null  object\n 7   7       11167 non-null  object\n 8   8       11167 non-null  object\n 9   9       11167 non-null  object\ndtypes: object(10)\nmemory usage: 959.7+ KB\nNone\n      0  1  2  3  4  5  6  7  8  9\n1523  0  0  0  0  0  1  0  0  0  0\n1467  0  0  0  0  1  0  0  0  0  0\n1380  0  0  0  0  0  0  1  0  0  0\n2150  1  1  0  1  1  0  0  0  0  0\n1034  0  0  0  0  0  1  0  0  0  0\n817   0  1  0  0  0  0  0  0  0  0\n292   0  0  0  0  1  0  0  1  0  0\n183   1  1  0  1  0  0  0  0  0  0\n431   0  0  1  0  0  0  0  0  0  0\n340   0  0  0  0  0  0  1  0  0  0\n951   0  0  0  0  0  0  1  0  0  0\n1879  1  1  0  1  0  0  0  0  0  0\n697   0  0  0  0  0  0  1  0  0  0\n432   0  0  0  0  1  0  0  0  0  0\n110   0  0  0  0  0  0  1  0  0  0\n58    0  0  0  0  0  0  0  0  1  1\n1314  1  0  0  1  0  1  0  0  0  0\n1477  0  0  0  0  1  0  0  0  0  0\n1791  0  0  0  0  1  0  0  0  0  0\n1083  0  0  0  0  0  0  1  0  0  0\n","output_type":"stream"}]},{"source":"<h3>Setup the class and run the fit command to find the estimated parameters from the test dataset.</h3>","metadata":{},"id":"24304506-f0c6-48f6-89c7-dba5653144d3","cell_type":"markdown"},{"source":"# Initialize algorithm\nlca = LatentClassAnalysis(n_classes=8, learning_rate_threshold=0.00001,\n                          max_iterations=4000, verbose_level=4, random_state=seed)\n# Fit model to the training data\nlca.fit(training_data)","metadata":{"outputsMetadata":{"0":{"height":212,"type":"stream"}}},"id":"0ad5bc0d-8562-4b60-862d-7351a6f690cc","cell_type":"code","execution_count":4,"outputs":[{"name":"stdout","text":"\n===================================================== fit run timings\navg run time per loop:  0.22784605804306676\ntotal run time:  216.90944725699956\n\n===================================================== fit state\niteration count: 953\nmax_iterations_hit: False\nlearning_rate_threshold_hit: True\nn_classes: 8\n","output_type":"stream"}]},{"source":"<h3>Print the results and compared the estimated parameters to the original test data parameters.</h3>","metadata":{},"id":"18921028-211c-4b53-8d71-fb420cd263f4","cell_type":"markdown"},{"source":"print(\"\\n==============================================================================================\")\nprint('\\ntrue_population_to_class_probability from test data')\nprint(true_population_to_class_probability)\n\nprint(\"\\n==============================================================================================\")\nprint('\\ntrue_variable_to_class_probability from test data')\npd.set_option('display.width', 320)\npd.set_option('display.max_columns', variable_count)\nprint(pd.DataFrame(true_variable_to_class_probability))\n\nprint(\"\\n==============================================================================================\")\nprint('\\npopulation_to_class_probability from model')\nprint(np.array2string(lca.population_to_class_probability, max_line_width=320, precision=4))\nprint('\\nvariable_to_class_probability from model')\nvariable_to_class_probability_to_print = []\nfor lst in lca.variable_to_class_probability:\n    variable_to_class_probability_to_print.append([round(val, 4) for val in lst])\npd.set_option('display.width', 320)\npd.set_option('display.max_columns', variable_count)\nprint(pd.DataFrame(variable_to_class_probability_to_print))\n\npredicted = lca.predict(training_data)\ncol_names = ['C' + str(c) for c in range(training_data.shape[1])] + ['pred']\ndf = pd.DataFrame(np.column_stack((training_data, predicted)), columns=col_names)\ngroups = df.groupby(['pred'])\nclass_probs = []\nfor g in groups:\n    col_probs = []\n    for col in col_names:\n        if col == 'pred':\n            continue\n        percentage_of_rows_this_column_equals_one = g[1][col].sum() / g[1].shape[0]\n        col_probs.append(percentage_of_rows_this_column_equals_one)\n    class_probs.append((col_probs, g[0]))\n\nprint(\"\\n==============================================================================================\")\nprint(\n    \"From training data For each predicted cluster the percentage of rows for each column in the data that has a 1 value\")\nvalues_to_print = []\nfor cp in class_probs:\n    values_to_print.append([round(val, 4) for val in cp[0]])\n    # print(cp[1], [round(val,4) for val in cp[0]])\npd.set_option('display.width', 320)\npd.set_option('display.max_columns', variable_count)\nprint(pd.DataFrame(values_to_print))","metadata":{"outputsMetadata":{"0":{"height":616,"type":"stream"}}},"id":"22f05eb6-8292-4f59-a120-be95d4b8521a","cell_type":"code","execution_count":5,"outputs":[{"name":"stdout","text":"\n==============================================================================================\n\ntrue_population_to_class_probability from test data\n[0.05, 0.08, 0.22, 0.14, 0.2, 0.14, 0.12, 0.05]\n\n==============================================================================================\n\ntrue_variable_to_class_probability from test data\n      0     1     2     3     4     5     6     7     8     9\n0  0.01  0.40  0.90  0.02  0.01  0.01  0.01  0.01  0.01  0.01\n1  0.04  0.90  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01\n2  0.90  0.90  0.05  0.90  0.01  0.01  0.01  0.01  0.01  0.01\n3  0.01  0.02  0.30  0.01  0.01  0.90  0.01  0.02  0.01  0.01\n4  0.01  0.01  0.01  0.03  0.80  0.01  0.01  0.02  0.01  0.01\n5  0.01  0.02  0.01  0.01  0.02  0.01  0.90  0.01  0.01  0.01\n6  0.01  0.01  0.01  0.03  0.90  0.01  0.01  0.90  0.01  0.01\n7  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.90  0.90\n\n==============================================================================================\n\npopulation_to_class_probability from model\n[0.104  0.1345 0.2364 0.0384 0.0542 0.0489 0.3007 0.0829]\n\nvariable_to_class_probability from model\n        0       1       2       3       4       5       6       7       8       9\n0  0.0204  1.0000  0.2012  0.0452  0.0031  0.0292  0.0184  0.0095  0.0104  0.0094\n1  0.0074  0.0168  0.0000  0.0105  0.0212  0.0128  1.0000  0.0098  0.0100  0.0095\n2  0.9153  0.8933  0.0445  0.8915  0.0096  0.0065  0.0081  0.0097  0.0095  0.0105\n3  0.0000  0.0000  0.0000  0.0370  0.0000  1.0000  0.0000  0.0000  0.0000  0.0000\n4  0.0078  0.0020  0.0104  0.0086  0.0061  0.0081  0.0080  0.0091  0.8899  0.8843\n5  0.0028  0.0385  1.0000  0.0190  0.0111  0.2685  0.0410  0.0033  0.0136  0.0110\n6  0.0081  0.0091  0.0099  0.0359  0.9560  0.0062  0.0097  0.3991  0.0083  0.0087\n7  0.0073  0.0000  0.3281  0.0000  0.0113  1.0000  0.0001  0.0303  0.0135  0.0114\n\n==============================================================================================\nFrom training data For each predicted cluster the percentage of rows for each column in the data that has a 1 value\n        0       1       2       3       4       5       6       7       8       9\n0  0.0018  1.0000  0.2305  0.0237  0.0009  0.0368  0.0044  0.0105  0.0105  0.0096\n1  0.0078  0.0260  0.0000  0.0098  0.0319  0.0124  1.0000  0.0098  0.0104  0.0098\n2  0.9135  0.8929  0.0408  0.8929  0.0105  0.0056  0.0086  0.0094  0.0094  0.0101\n3  0.0000  0.0000  0.0000  1.0000  0.0000  1.0000  0.0000  0.0000  0.0000  0.0000\n4  0.0082  0.0016  0.0082  0.0099  0.0049  0.0066  0.0066  0.0148  0.8898  0.8832\n5  0.0000  0.0000  1.0000  0.0281  0.0026  0.0205  0.0563  0.0051  0.0153  0.0128\n6  0.0075  0.0090  0.0105  0.0339  0.9604  0.0081  0.0042  0.4001  0.0075  0.0081\n7  0.0048  0.0000  0.2981  0.0000  0.0027  1.0000  0.0000  0.0198  0.0095  0.0082\n","output_type":"stream"}]},{"source":"<h3>Run the test data through again this time without specifying the number of latent classes so the class automatically guesses how many latent classes there are.</h3>","metadata":{},"id":"a4a97621-ae1c-465a-9c21-4efdda718af7","cell_type":"markdown"},{"source":"# Initialize algorithm\nlca = LatentClassAnalysis(class_count_method='average_class_count', learning_rate_threshold=0.00001,\n                          max_iterations=4000, verbose_level=4, random_state=seed)\n# Fit model to the training data\nlca.fit(training_data)","metadata":{"outputsMetadata":{"0":{"height":616,"type":"stream"}}},"id":"a8688a27-66ae-4ee7-8454-834d4e84288e","cell_type":"code","execution_count":6,"outputs":[{"name":"stdout","text":"** Calculating classes by prevalence **\n\n===================================================== Class keys and counts\n(0, 0, 0, 0, 1, 0, 0, 0, 0, 0) 1829\n(1, 1, 0, 1, 0, 0, 0, 0, 0, 0) 1762\n(0, 0, 0, 0, 0, 0, 1, 0, 0, 0) 1367\n(0, 0, 0, 0, 1, 0, 0, 1, 0, 0) 1095\n(0, 0, 0, 0, 0, 1, 0, 0, 0, 0) 991\n(0, 1, 0, 0, 0, 0, 0, 0, 0, 0) 822\n(0, 0, 0, 0, 0, 0, 0, 0, 1, 1) 445\n(0, 0, 1, 0, 0, 1, 0, 0, 0, 0) 412\n(0, 0, 1, 0, 0, 0, 0, 0, 0, 0) 346\n(1, 1, 0, 0, 0, 0, 0, 0, 0, 0) 235\n(0, 1, 1, 0, 0, 0, 0, 0, 0, 0) 218\n(1, 0, 0, 1, 0, 0, 0, 0, 0, 0) 191\n(0, 1, 0, 1, 0, 0, 0, 0, 0, 0) 181\n(0, 0, 0, 0, 0, 0, 0, 1, 0, 0) 128\n(1, 1, 1, 1, 0, 0, 0, 0, 0, 0) 81\n(0, 0, 0, 0, 0, 0, 0, 0, 1, 0) 64\n(0, 0, 0, 1, 1, 0, 0, 0, 0, 0) 64\n(0, 0, 0, 0, 0, 0, 0, 0, 0, 1) 62\n(0, 0, 0, 0, 1, 0, 1, 0, 0, 0) 43\n(0, 0, 0, 1, 1, 0, 0, 1, 0, 0) 42\n(0, 1, 0, 0, 0, 0, 1, 0, 0, 0) 40\n(0, 0, 0, 1, 0, 0, 0, 0, 0, 0) 38\n(1, 0, 0, 0, 0, 0, 0, 0, 0, 0) 28\n(0, 1, 1, 1, 0, 0, 0, 0, 0, 0) 22\n(0, 1, 0, 0, 0, 1, 0, 0, 0, 0) 22\n(0, 0, 1, 0, 1, 0, 0, 0, 0, 0) 21\n(0, 1, 0, 0, 1, 0, 0, 0, 0, 0) 19\n(1, 1, 0, 1, 0, 0, 0, 0, 0, 1) 19\n(1, 1, 0, 1, 0, 0, 0, 1, 0, 0) 19\n(0, 0, 0, 0, 0, 1, 0, 1, 0, 0) 19\n(0, 0, 0, 0, 1, 1, 0, 0, 0, 0) 17\n(0, 0, 0, 1, 0, 1, 0, 0, 0, 0) 16\n(0, 0, 0, 0, 1, 0, 0, 0, 0, 1) 16\n(1, 1, 0, 1, 0, 0, 1, 0, 0, 0) 15\n(0, 0, 0, 0, 0, 1, 1, 0, 0, 0) 15\n(1, 0, 0, 0, 1, 0, 0, 0, 0, 0) 15\n(0, 0, 0, 1, 0, 0, 1, 0, 0, 0) 15\n(0, 0, 1, 0, 0, 0, 1, 0, 0, 0) 15\n(0, 0, 0, 0, 1, 0, 0, 0, 1, 0) 14\n(0, 0, 0, 0, 0, 0, 1, 1, 0, 0) 14\n(0, 0, 0, 0, 0, 0, 1, 0, 0, 1) 13\n(0, 1, 1, 0, 0, 1, 0, 0, 0, 0) 12\n(0, 0, 1, 0, 1, 0, 0, 1, 0, 0) 12\n(1, 1, 0, 1, 0, 0, 0, 0, 1, 0) 11\n(1, 0, 1, 1, 0, 0, 0, 0, 0, 0) 11\n(0, 0, 0, 0, 0, 0, 1, 0, 1, 0) 11\n(0, 0, 0, 0, 1, 0, 1, 1, 0, 0) 11\n(0, 1, 0, 0, 0, 0, 0, 0, 1, 0) 10\n(0, 1, 0, 0, 1, 0, 0, 1, 0, 0) 10\n(1, 1, 0, 1, 1, 0, 0, 0, 0, 0) 10\n(0, 0, 1, 1, 0, 0, 0, 0, 0, 0) 9\n(0, 1, 0, 0, 0, 0, 0, 1, 0, 0) 9\n(1, 1, 0, 1, 0, 1, 0, 0, 0, 0) 9\n(0, 0, 0, 0, 1, 0, 0, 1, 0, 1) 9\n(1, 1, 1, 0, 0, 0, 0, 0, 0, 0) 8\n(0, 1, 0, 0, 0, 0, 0, 0, 0, 1) 8\n(1, 0, 0, 0, 0, 0, 1, 0, 0, 0) 8\n(0, 0, 0, 0, 0, 1, 0, 0, 1, 0) 8\n(0, 0, 1, 0, 0, 1, 0, 1, 0, 0) 7\n(1, 0, 0, 0, 1, 0, 0, 1, 0, 0) 7\n(0, 0, 0, 0, 1, 1, 0, 1, 0, 0) 7\n(1, 0, 0, 1, 1, 0, 0, 0, 0, 0) 6\n(0, 0, 0, 0, 0, 1, 0, 0, 0, 1) 6\n(0, 0, 1, 0, 0, 1, 1, 0, 0, 0) 6\n(0, 0, 0, 0, 1, 0, 0, 1, 1, 0) 6\n(0, 0, 1, 0, 0, 0, 0, 0, 0, 1) 5\n(0, 0, 1, 0, 0, 0, 0, 0, 1, 0) 5\n(0, 1, 0, 1, 1, 0, 0, 0, 0, 0) 5\n(0, 0, 1, 0, 0, 1, 0, 0, 0, 1) 5\n(0, 0, 1, 0, 0, 1, 0, 0, 1, 0) 5\n(0, 0, 0, 0, 0, 0, 0, 1, 1, 0) 5\n(0, 0, 1, 0, 0, 0, 0, 0, 1, 1) 5\n(1, 0, 0, 1, 0, 0, 0, 1, 0, 0) 4\n(0, 1, 0, 1, 0, 0, 1, 0, 0, 0) 4\n(1, 0, 0, 0, 0, 1, 0, 0, 0, 0) 4\n(0, 0, 0, 1, 0, 0, 0, 1, 0, 0) 4\n(0, 0, 0, 0, 0, 0, 1, 0, 1, 1) 4\n(1, 0, 0, 0, 0, 0, 0, 0, 1, 1) 4\n(0, 0, 0, 0, 0, 1, 0, 0, 1, 1) 4\n(0, 1, 1, 0, 0, 0, 1, 0, 0, 0) 3\n(1, 1, 1, 1, 0, 0, 0, 0, 1, 0) 3\n(1, 1, 0, 0, 0, 1, 0, 0, 0, 0) 3\n(1, 0, 0, 1, 0, 0, 0, 0, 1, 0) 3\n(0, 1, 0, 1, 0, 1, 0, 0, 0, 0) 3\n(1, 1, 0, 0, 0, 0, 0, 0, 0, 1) 3\n(1, 0, 1, 0, 0, 1, 0, 0, 0, 0) 3\n(0, 0, 1, 0, 1, 1, 0, 0, 0, 0) 3\n(0, 0, 0, 0, 0, 0, 0, 1, 0, 1) 3\n(0, 0, 0, 0, 1, 0, 0, 0, 1, 1) 3\n(0, 0, 0, 1, 0, 0, 0, 0, 1, 1) 3\n(0, 1, 0, 0, 0, 0, 0, 1, 0, 1) 2\n(0, 0, 1, 0, 0, 0, 0, 1, 0, 0) 2\n(1, 0, 1, 0, 0, 0, 0, 0, 0, 0) 2\n(0, 1, 1, 0, 0, 0, 0, 0, 1, 0) 2\n(1, 1, 0, 0, 1, 0, 0, 0, 0, 0) 2\n(1, 1, 1, 0, 0, 1, 0, 0, 0, 0) 2\n(1, 1, 0, 1, 1, 0, 0, 0, 0, 1) 2\n(1, 1, 0, 0, 0, 0, 0, 0, 1, 0) 2\n(0, 0, 0, 1, 0, 0, 0, 0, 1, 0) 2\n(0, 0, 0, 0, 0, 1, 1, 0, 1, 0) 2\n(1, 0, 0, 0, 1, 0, 1, 0, 0, 0) 2\n(0, 0, 0, 0, 1, 0, 1, 0, 1, 0) 2\n(0, 1, 1, 0, 0, 0, 0, 0, 0, 1) 1\n(0, 1, 1, 0, 0, 0, 0, 1, 0, 0) 1\n(0, 1, 1, 0, 1, 0, 0, 0, 0, 0) 1\n(1, 1, 1, 0, 0, 0, 1, 0, 0, 0) 1\n(1, 1, 0, 0, 0, 0, 1, 0, 0, 0) 1\n(1, 1, 0, 0, 1, 0, 0, 0, 1, 0) 1\n(1, 1, 1, 1, 0, 0, 0, 1, 0, 1) 1\n(1, 0, 0, 1, 0, 0, 1, 0, 0, 0) 1\n(0, 1, 0, 1, 0, 0, 0, 0, 1, 0) 1\n(1, 1, 1, 1, 1, 0, 0, 1, 0, 0) 1\n(0, 1, 0, 1, 1, 0, 0, 0, 1, 0) 1\n(1, 1, 1, 1, 0, 1, 0, 0, 1, 0) 1\n(0, 1, 0, 1, 0, 0, 0, 0, 0, 1) 1\n(1, 1, 0, 1, 0, 0, 1, 0, 1, 0) 1\n(1, 0, 0, 1, 0, 0, 0, 0, 0, 1) 1\n(1, 0, 0, 1, 0, 1, 0, 0, 0, 0) 1\n(0, 1, 0, 1, 0, 1, 1, 0, 0, 0) 1\n(1, 1, 0, 1, 0, 1, 0, 0, 1, 0) 1\n(0, 0, 1, 0, 0, 1, 0, 1, 0, 1) 1\n(1, 0, 0, 0, 0, 1, 1, 0, 0, 0) 1\n(0, 0, 1, 1, 1, 1, 0, 0, 0, 0) 1\n(0, 0, 1, 0, 1, 1, 0, 1, 0, 0) 1\n(0, 0, 0, 0, 1, 1, 0, 0, 1, 0) 1\n(0, 0, 1, 1, 0, 1, 0, 0, 0, 0) 1\n(0, 1, 1, 1, 0, 1, 0, 0, 0, 0) 1\n(0, 0, 0, 0, 1, 1, 0, 0, 0, 1) 1\n(1, 0, 0, 0, 1, 1, 0, 0, 0, 0) 1\n(0, 0, 1, 0, 1, 0, 0, 0, 1, 0) 1\n(0, 0, 0, 0, 1, 1, 1, 0, 0, 0) 1\n(1, 0, 0, 0, 1, 0, 0, 0, 1, 0) 1\n(0, 0, 0, 1, 1, 0, 0, 0, 0, 1) 1\n(0, 0, 0, 0, 0, 0, 1, 1, 0, 1) 1\n(0, 0, 1, 0, 0, 0, 1, 0, 1, 0) 1\n(0, 0, 0, 1, 1, 0, 1, 0, 0, 0) 1\n(0, 1, 0, 0, 0, 1, 1, 0, 0, 0) 1\n(0, 0, 0, 0, 1, 0, 1, 0, 0, 1) 1\n(0, 0, 1, 0, 1, 0, 0, 1, 1, 0) 1\n(1, 0, 0, 0, 1, 0, 1, 1, 0, 0) 1\n(0, 0, 0, 0, 0, 1, 0, 1, 1, 0) 1\n(0, 0, 0, 0, 1, 0, 1, 1, 1, 0) 1\n(0, 1, 0, 1, 1, 0, 0, 1, 0, 0) 1\n(1, 0, 0, 0, 0, 0, 0, 0, 0, 1) 1\n(0, 1, 0, 0, 0, 0, 0, 0, 1, 1) 1\n(0, 0, 0, 0, 0, 0, 0, 1, 1, 1) 1\n(0, 0, 0, 1, 0, 0, 0, 0, 0, 1) 1\n(1, 0, 0, 0, 0, 0, 1, 0, 1, 0) 1\n\n===================================================== Sorted diff counts between class keys\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 6, 10, 10, 17, 17, 19, 27, 33, 47, 53, 66, 104, 111, 169, 272, 377, 395]\n\n===================================================== least_probable_diff stats\n      probability  mean  std\ndiff                        \n0        0.398942   0.0  1.0\n0        1.000000   0.0  0.0\n0        1.000000   0.0  0.0\n0        1.000000   0.0  0.0\n0        1.000000   0.0  0.0\n       probability    mean      std\ndiff                               \n0     1.979930e-01  0.4773   1.9558\n17    0.000000e+00  0.4737   1.9489\n47    0.000000e+00  0.5970   2.4068\n53    0.000000e+00  0.9407   4.6456\n10    2.492904e-02  1.3235   6.4190\n27    2.266000e-05  1.3869   6.4381\n17    4.399240e-03  1.5725   6.7726\n111   0.000000e+00  1.6835   6.8730\n66    1.000000e-08  2.4643  11.4738\n33    1.840770e-03  2.9149  12.6151\n377   0.000000e+00  3.1268  12.8199\n169   9.000000e-08  5.7413  33.6728\n104   3.008800e-04  6.8750  36.1910\n272   0.000000e+00  7.5448  36.9508\n395   0.000000e+00  9.3562  42.7989\n\n===================================================== Classes by prevalence\nlargest_diff\n((1, 1, 0, 1, 0, 0, 0, 0, 0, 0), 1762)\n((0, 0, 0, 0, 1, 0, 0, 0, 0, 0), 1829)\nleast_probable_diff\n((0, 1, 0, 0, 0, 0, 0, 0, 0, 0), 822)\n((0, 0, 0, 0, 0, 1, 0, 0, 0, 0), 991)\n((0, 0, 0, 0, 1, 0, 0, 1, 0, 0), 1095)\n((0, 0, 0, 0, 0, 0, 1, 0, 0, 0), 1367)\n((1, 1, 0, 1, 0, 0, 0, 0, 0, 0), 1762)\n((0, 0, 0, 0, 1, 0, 0, 0, 0, 0), 1829)\ndiff_percentile\n((0, 1, 0, 0, 0, 0, 0, 0, 0, 0), 822)\n((0, 0, 0, 0, 0, 1, 0, 0, 0, 0), 991)\n((0, 0, 0, 0, 1, 0, 0, 1, 0, 0), 1095)\n((0, 0, 0, 0, 0, 0, 1, 0, 0, 0), 1367)\n((1, 1, 0, 1, 0, 0, 0, 0, 0, 0), 1762)\n((0, 0, 0, 0, 1, 0, 0, 0, 0, 0), 1829)\nleast_probable_percentile\n((0, 0, 0, 0, 0, 0, 0, 1, 0, 0), 128)\n((0, 1, 0, 1, 0, 0, 0, 0, 0, 0), 181)\n((1, 0, 0, 1, 0, 0, 0, 0, 0, 0), 191)\n((0, 1, 1, 0, 0, 0, 0, 0, 0, 0), 218)\n((1, 1, 0, 0, 0, 0, 0, 0, 0, 0), 235)\n((0, 0, 1, 0, 0, 0, 0, 0, 0, 0), 346)\n((0, 0, 1, 0, 0, 1, 0, 0, 0, 0), 412)\n((0, 0, 0, 0, 0, 0, 0, 0, 1, 1), 445)\n((0, 1, 0, 0, 0, 0, 0, 0, 0, 0), 822)\n((0, 0, 0, 0, 0, 1, 0, 0, 0, 0), 991)\n((0, 0, 0, 0, 1, 0, 0, 1, 0, 0), 1095)\n((0, 0, 0, 0, 0, 0, 1, 0, 0, 0), 1367)\n((1, 1, 0, 1, 0, 0, 0, 0, 0, 0), 1762)\n((0, 0, 0, 0, 1, 0, 0, 0, 0, 0), 1829)\n\n===================================================== Class count method stats\nmax count diff between class keys: 395\ndiff_percentile: 99 %\nleast_probable_percentile: 1 %\nmedian class count: 6\naverage class count: 7\nlargest_diff class count: 2\nleast_probable_diff class count: 6\ndiff_percentile class count: 6\nleast_probable_percentile class count: 14\n\n===================================================== fit run timings\navg run time per loop:  0.20206577067451256\ntotal run time:  690.0546068534604\n\n===================================================== fit state\niteration count: 3416\nmax_iterations_hit: False\nlearning_rate_threshold_hit: True\nn_classes: 7\n","output_type":"stream"}]},{"source":"<h3>Print the results and compared the estimated parameters to the original test data parameters.</h3>","metadata":{},"id":"10ac922d-1f13-4bb0-94b7-9d6f5ae2f7e8","cell_type":"markdown"},{"source":"print(\"\\n==============================================================================================\")\nprint('\\ntrue_population_to_class_probability from test data')\nprint(true_population_to_class_probability)\n\nprint(\"\\n==============================================================================================\")\nprint('\\ntrue_variable_to_class_probability from test data')\npd.set_option('display.width', 320)\npd.set_option('display.max_columns', variable_count)\nprint(pd.DataFrame(true_variable_to_class_probability))\n\nprint(\"\\n==============================================================================================\")\nprint('\\npopulation_to_class_probability from model')\nprint(np.array2string(lca.population_to_class_probability, max_line_width=320, precision=4))\nprint('\\nvariable_to_class_probability from model')\nvariable_to_class_probability_to_print = []\nfor lst in lca.variable_to_class_probability:\n    variable_to_class_probability_to_print.append([round(val, 4) for val in lst])\npd.set_option('display.width', 320)\npd.set_option('display.max_columns', variable_count)\nprint(pd.DataFrame(variable_to_class_probability_to_print))\n\npredicted = lca.predict(training_data)\ncol_names = ['C' + str(c) for c in range(training_data.shape[1])] + ['pred']\ndf = pd.DataFrame(np.column_stack((training_data, predicted)), columns=col_names)\ngroups = df.groupby(['pred'])\nclass_probs = []\nfor g in groups:\n    col_probs = []\n    for col in col_names:\n        if col == 'pred':\n            continue\n        percentage_of_rows_this_column_equals_one = g[1][col].sum() / g[1].shape[0]\n        col_probs.append(percentage_of_rows_this_column_equals_one)\n    class_probs.append((col_probs, g[0]))\n\nprint(\"\\n==============================================================================================\")\nprint(\n    \"From training data For each predicted cluster the percentage of rows for each column in the data that has a 1 value\")\nvalues_to_print = []\nfor cp in class_probs:\n    values_to_print.append([round(val, 4) for val in cp[0]])\n    # print(cp[1], [round(val,4) for val in cp[0]])\npd.set_option('display.width', 320)\npd.set_option('display.max_columns', variable_count)\nprint(pd.DataFrame(values_to_print))","metadata":{},"id":"28bf8a36-6caf-46ed-96db-59008e840305","cell_type":"code","execution_count":7,"outputs":[{"name":"stdout","text":"\n==============================================================================================\n\ntrue_population_to_class_probability from test data\n[0.05, 0.08, 0.22, 0.14, 0.2, 0.14, 0.12, 0.05]\n\n==============================================================================================\n\ntrue_variable_to_class_probability from test data\n      0     1     2     3     4     5     6     7     8     9\n0  0.01  0.40  0.90  0.02  0.01  0.01  0.01  0.01  0.01  0.01\n1  0.04  0.90  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01\n2  0.90  0.90  0.05  0.90  0.01  0.01  0.01  0.01  0.01  0.01\n3  0.01  0.02  0.30  0.01  0.01  0.90  0.01  0.02  0.01  0.01\n4  0.01  0.01  0.01  0.03  0.80  0.01  0.01  0.02  0.01  0.01\n5  0.01  0.02  0.01  0.01  0.02  0.01  0.90  0.01  0.01  0.01\n6  0.01  0.01  0.01  0.03  0.90  0.01  0.01  0.90  0.01  0.01\n7  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.90  0.90\n\n==============================================================================================\n\npopulation_to_class_probability from model\n[0.1196 0.0141 0.2888 0.0157 0.261  0.248  0.0527]\n\nvariable_to_class_probability from model\n        0       1       2       3       4       5       6       7       8       9\n0  0.0000  0.0286  0.0073  0.0115  0.0000  0.0000  1.0000  0.0112  0.0000  0.0101\n1  0.0000  0.0517  0.0000  0.0228  0.0000  0.0929  0.0000  1.0000  0.0374  0.0305\n2  0.0082  0.0092  0.0109  0.0346  1.0000  0.0078  0.0123  0.3733  0.0077  0.0085\n3  0.0650  0.0000  0.0245  0.0000  0.1322  0.0796  1.0000  0.0000  0.0918  0.0000\n4  0.0057  0.3520  0.3618  0.0163  0.0045  0.5227  0.0090  0.0081  0.0126  0.0107\n5  0.8781  0.9001  0.0468  0.8631  0.0095  0.0053  0.0084  0.0093  0.0095  0.0104\n6  0.0078  0.0028  0.0109  0.0079  0.0062  0.0082  0.0083  0.0017  0.8993  0.8978\n\n==============================================================================================\nFrom training data For each predicted cluster the percentage of rows for each column in the data that has a 1 value\n        0       1       2       3       4       5       6       7       8       9\n0  0.0000  0.0273  0.0102  0.0102  0.0000  0.0000  1.0000  0.0102  0.0000  0.0096\n1  0.0000  0.0643  0.0000  0.0234  0.0000  0.1170  0.0000  1.0000  0.0351  0.0292\n2  0.0077  0.0092  0.0111  0.0335  1.0000  0.0086  0.0178  0.3705  0.0077  0.0086\n3  0.2727  0.0000  0.0227  0.0000  0.1136  0.4318  1.0000  0.0000  0.3864  0.0000\n4  0.0047  0.3767  0.3574  0.0109  0.0017  0.5134  0.0037  0.0037  0.0102  0.0088\n5  0.9059  0.8943  0.0480  0.8955  0.0104  0.0045  0.0086  0.0093  0.0093  0.0100\n6  0.0083  0.0017  0.0083  0.0100  0.0050  0.0067  0.0067  0.0017  0.8933  0.8900\n","output_type":"stream"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}